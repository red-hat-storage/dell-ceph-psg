# Please do not change this file directly since it is managed by Ansible and will be overwritten

[global]
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
cephx require signatures = True # Kernel RBD does NOT support signatures!
cephx cluster require signatures = True
cephx service require signatures = False
fsid = 0a845f6e-ea73-4dc5-b0fd-c029e57603be
max open files = 131072
osd pool default pg num = 2048
osd pool default pgp num = 2048
osd pool default size = 3
osd pool default min size = 2
osd pool default crush rule = 0
# Disable in-memory logs
debug_lockdep = 0/0
debug_context = 0/0
debug_crush = 0/0
debug_buffer = 0/0
debug_timer = 0/0
debug_filer = 0/0
debug_objecter = 0/0
debug_rados = 0/0
debug_rbd = 0/0
debug_journaler = 0/0
debug_objectcatcher = 0/0
debug_client = 0/0
debug_osd = 0/0
debug_optracker = 0/0
debug_objclass = 0/0
debug_filestore = 0/0
debug_journal = 0/0
debug_ms = 0/0
debug_monc = 0/0
debug_tp = 0/0
debug_auth = 0/0
debug_finisher = 0/0
debug_heartbeatmap = 0/0
debug_perfcounter = 0/0
debug_asok = 0/0
debug_throttle = 0/0
debug_mon = 0/0
debug_paxos = 0/0
debug_rgw = 0/0

[client]
rbd cache = false
rbd cache writethrough until flush = false
rbd concurrent management ops = 20
admin socket = /var/run/ceph/rbd-clients/$cluster-$type.$id.$pid.$cctid.asok # must be writable by QEMU and allowed by SELinux or AppArmor
log file = /var/log/rbd-clients/qemu-guest-$pid.log # must be writable by QEMU and allowed by SELinux or AppArmor
rbd default map options = rw
rbd default features = 3 # sum features digits
rbd default format = 2

[mon]
mon osd down out interval = 600
mon osd min down reporters = 7
mon clock drift allowed = 0.15
mon clock drift warn backoff = 30
mon osd full ratio = 0.95
mon osd nearfull ratio = 0.85
mon osd report timeout = 300
mon pg warn max per osd = 0
mon osd allow primary affinity = true
mon pg warn max object skew = 10

[mon.r630-01]
host = r630-01
mon addr = 192.168.100.101
[mon.r630-02]
host = r630-02
mon addr = 192.168.100.102
[mon.r630-03]
host = r630-03
mon addr = 192.168.100.103

[osd]
osd mkfs type = xfs
osd mkfs options xfs = -f -i size=2048
osd mount options xfs = noatime,largeio,inode64,swalloc
osd journal size = 10000
cluster_network = 192.168.200.0/24
public_network = 192.168.100.0/24
osd mon heartbeat interval = 30
# Performance tuning
filestore merge threshold = 40
filestore split multiple = 8
osd op threads = 8
filestore op threads = 8
filestore max sync interval = 5
filestore min sync interval = 0.01
osd max scrubs = 1
osd scrub begin hour = 0
osd scrub end hour = 24
# Recovery tuning
osd recovery max active = 5
osd max backfills = 2
osd recovery op priority = 2
osd recovery max chunk = 1048576
osd recovery threads = 1
osd objectstore = filestore
osd crush update on start = true
# Deep scrub impact
osd scrub sleep = 0.1
osd disk thread ioprio class = idle
osd disk thread ioprio priority = 0
osd scrub chunk max = 5
osd deep scrub stride = 1048576


[client.rgw.r630-01]
host = r630-01
keyring = /var/lib/ceph/radosgw/ceph-rgw.r630-01/keyring
rgw socket path = /tmp/radosgw-r630-01.sock
log file = /var/log/ceph/radosgw-r630-01.log
rgw data = /var/lib/ceph/radosgw/ceph-rgw.r630-01
rgw frontends = civetweb port=8080
[client.rgw.r630-02]
host = r630-02
keyring = /var/lib/ceph/radosgw/ceph-rgw.r630-02/keyring
rgw socket path = /tmp/radosgw-r630-02.sock
log file = /var/log/ceph/radosgw-r630-02.log
rgw data = /var/lib/ceph/radosgw/ceph-rgw.r630-02
rgw frontends = civetweb port=8080
[client.rgw.r630-03]
host = r630-03
keyring = /var/lib/ceph/radosgw/ceph-rgw.r630-03/keyring
rgw socket path = /tmp/radosgw-r630-03.sock
log file = /var/log/ceph/radosgw-r630-03.log
rgw data = /var/lib/ceph/radosgw/ceph-rgw.r630-03
rgw frontends = civetweb port=8080

